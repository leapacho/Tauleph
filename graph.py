from langchain_google_genai import ChatGoogleGenerativeAI
#from langchain_groq import ChatGroq
from langchain_core.messages import AIMessage
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.graph import CompiledGraph
from langgraph.graph.message import add_messages 
from langchain_community.utilities import SearxSearchWrapper
from langchain_community.tools.searx_search.tool import SearxSearchResults
from langgraph.prebuilt import ToolNode, tools_condition
from textwrap import TextWrapper
#import sqlite3
#from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv

load_dotenv() #Loads environment variables.

# conn = sqlite3.connect("checkpoints.sqlite", check_same_thread=False)
memory = MemorySaver() #SqliteSaver is a class that intefaces sqlite3.
searx_search = SearxSearchWrapper(searx_host="http://localhost:32787") #SearxSearchWrapper is a class that allows you to interact with the Searx API.
searx_tool = SearxSearchResults(wrapper=searx_search, num_results=10) #SearxSearchResults is a class that allows you to get search results from Searx.

class State(TypedDict): 
    messages: Annotated[list, add_messages] #Creates the state. The state is a dictionary that contains the messages.

def setup_graph(input_model: str):
    graph_builder = StateGraph(State) #StateGraph is a class that creates a graph with the state.

    llm = ChatGoogleGenerativeAI(model=input_model)
    tools = [searx_tool]
     
    llm_with_tools = llm.bind_tools(tools)

    def chatbot(state: State):
        return {"messages": [llm.invoke(state["messages"])]} #The chatbot function takes the state as input and returns the messages generated by the LLM with tools.
    
    def split_text_into_chunks(state: State, chunk_size: int = 50) -> list:
        w = TextWrapper(width=chunk_size,break_long_words=True,replace_whitespace=False)
        chunked_lines=w.wrap(state["messages"][-1].content)
        state["messages"][-1] = AIMessage(content=chunked_lines)
        return state

    def should_split_message(state: State):
        if len(state["messages"][-1].content) > 2000:
            return "split_output"
        else:
            return END
        
    def should_split_node(state):
        """
        Placeholder node to act as the source for the should_split_message condition.
        You could add logging or minor state checks here if desired.
        """
        message = state["messages"][-1].content
        state["messages"][-1] = AIMessage(content=[message]) # Convert the message into a list.
        return state # Or return {"messages": state['messages'], ...} etc.
    

    graph_builder.add_node("chatbot", chatbot) #Adds the chatbot node to the graph.
    graph_builder.add_node("tools", ToolNode(tools)) #Adds the tools node to the graph.
    graph_builder.add_node("split_output", split_text_into_chunks)
    graph_builder.add_node("should_split", should_split_node)
    graph_builder.set_entry_point("chatbot") #Sets the entry point of the graph to the chatbot node.

    graph_builder.add_conditional_edges(
        "chatbot",
        tools_condition, # The built-in function
        {
            "tools": "tools",  # If tools_condition returns "tools", go to "tools" node
            END: "should_split" # If tools_condition returns END, go to the "check_split" decision point
        }
    )
    graph_builder.add_conditional_edges(
        "should_split",         # Source is the route name defined above
        should_split_message,  # Your custom function decides the next step
            {
                "split_output": "split_output", # If should_split_message returns "split_output"
                END: END                      # If should_split_message returns END, go to the actual END
            }
        )
    graph_builder.add_edge("tools", "chatbot")
    graph_builder.add_edge("split_output", END)
    
    runnable = graph_builder.compile(checkpointer=memory) #Compiles the graph with the memory checkpointer.

    return runnable #Returns the runnable graph.

async def run_graph(graph: CompiledGraph, config, initial_messages=None) -> str:
    """
    Processes the input messages through the graph and returns the last message.
    """

    response = await graph.ainvoke({"messages": initial_messages}, config) if initial_messages else graph.invoke(None, config) 

    return response["messages"][-1].content


async def run_graph_mock(graph, config, initial_messages=None, char_limit=False):

    response = "Hello!!! This is a test." if not char_limit else "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"

    return response
